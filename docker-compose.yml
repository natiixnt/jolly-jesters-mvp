# Plik: docker-compose.yml
# usunieto 'version: "3.9"' (przestarzale)

services:
  postgres:
    image: postgres:15
    container_name: pilot_postgres
    restart: unless-stopped
    environment:
      POSTGRES_USER: pilot
      POSTGRES_PASSWORD: pilot
      POSTGRES_DB: pilotdb
      SELENIUM_HEADED: "true"
    ports:
      - "5433:5432"
    volumes:
      - pgdata:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U pilot -d pilotdb"]
      interval: 5s
      timeout: 5s
      retries: 5

  redis:
    image: redis:7
    container_name: pilot_redis
    restart: unless-stopped
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 5s
      retries: 5

  migrations:
    build:
      context: ./backend
    container_name: pilot_migrations
    working_dir: /app
    env_file:
      - ./backend/.env
    command: >
      bash -lc "cd /app && until pg_isready -h postgres -p 5432; do echo 'Waiting for Postgres...'; sleep 2; done; alembic -c alembic.ini upgrade head"
    depends_on:
      postgres:
        condition: service_healthy

  backend:
    build:
      context: ./backend
    container_name: pilot_backend
    working_dir: /app
    env_file:
      - ./backend/.env
    environment:
      - PROXY_LIST=${PROXY_LIST:-}
    command: >
      bash -lc "cd /app && uvicorn app.main:app --host 0.0.0.0 --port 8000"
    volumes:
      - workspace_data:/workspace
    ports:
      - "8000:8000"
    depends_on:
      migrations:
        condition: service_completed_successfully
      local_scraper:
        condition: service_healthy
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    extra_hosts:
      - "host.docker.internal:host-gateway"

  local_scraper:
    build:
      context: ./backend
      dockerfile: Dockerfile.local_scraper
      args:
        WITH_VNC: ${LOCAL_SCRAPER_WITH_VNC:-0}
    container_name: local_scraper
    restart: unless-stopped
    env_file:
      - ./backend/.env
    environment:
      - DISPLAY=:99
      - SELENIUM_HEADED=true
      - SELENIUM_USER_DATA_DIR=/data/chrome-profile
      - LOCAL_SCRAPER_ENABLE_VNC=${LOCAL_SCRAPER_ENABLE_VNC:-0}
      - LOCAL_SCRAPER_LISTING_TIMEOUT=50
      - LOCAL_SCRAPER_PAGELOAD_TIMEOUT=45
    ports:
      - "5050:5050"
      # Optional: noVNC (use SSH tunnel / firewall on VPS)
      - "127.0.0.1:6080:6080"
    volumes:
      - local_scraper_profile:/data/chrome-profile
    shm_size: 2gb # needed for Chrome
    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://localhost:5050/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s
      
  worker:
    build:
      context: ./backend
    container_name: pilot_worker
    env_file:
      - ./backend/.env
    environment:
      - PROXY_LIST=${PROXY_LIST:-}
    command: >
      sh -c "celery -A app.workers.tasks worker --loglevel=info -Q analysis --concurrency=2 --prefetch-multiplier=1 --max-tasks-per-child=20"
    volumes:
      - workspace_data:/workspace
    depends_on:
      migrations:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    extra_hosts:
      - "host.docker.internal:host-gateway"

  scraper_cloud_worker:
    build:
      context: ./backend
    container_name: pilot_scraper_cloud_worker
    env_file:
      - ./backend/.env
    environment:
      - PROXY_LIST=${PROXY_LIST:-}
      - WORKSPACE=/workspace
    command: >
      sh -c "celery -A app.workers.tasks worker --loglevel=info -Q scraper_cloud --concurrency=${SCRAPER_CLOUD_CONCURRENCY:-8} --prefetch-multiplier=1 --max-tasks-per-child=20"
    volumes:
      - workspace_data:/workspace
    depends_on:
      migrations:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    extra_hosts:
      - "host.docker.internal:host-gateway"
      
  scraper_worker:
    build:
      context: ./backend
    container_name: pilot_scraper_worker
    # --- POPRAWKA: dodalem env_file zeby wczytac proxy ---
    env_file:
      - ./backend/.env
    environment:
      - PROXY_LIST=${PROXY_LIST:-}
      - WORKSPACE=/workspace
    command: >
      sh -c "celery -A app.workers.tasks worker --loglevel=info -Q scraper_local --concurrency=1 --prefetch-multiplier=1 --max-tasks-per-child=20"
    volumes:
      - workspace_data:/workspace
    shm_size: 2gb # potrzebne dla selenium/chrome
    extra_hosts:
      - "host.docker.internal:host-gateway"
    dns:
      - 8.8.8.8
    depends_on:
      migrations:
        condition: service_completed_successfully
      local_scraper:
        condition: service_healthy
      postgres: 
        condition: service_healthy
      redis:
        condition: service_healthy
    extra_hosts:
      - "host.docker.internal:host-gateway"

  frontend:
    build:
      context: ./frontend
    container_name: pilot_frontend
    command: streamlit run app.py --server.port 8501 --server.address 0.0.0.0
    volumes:
      - ./frontend:/app
    ports:
      - "8501:8501"
    depends_on:
      - backend

volumes:
  pgdata:
  workspace_data: {}
  local_scraper_profile: {}
